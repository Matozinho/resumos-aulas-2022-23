# Introdução

## Como avaliar um algoritmo

  - Corretude
  - Eficiência Temporal
  - Eficiência Espacial

**Projeto de Algoritmos x Análise de Algoritmos**
  - Análise de Algoritmos:
    - Vantagens
  - Projetos de ALgoritmos
    - Podem levar em soluções melhores ou piores
    - Transformar para conquistar (ordena o vetor e pega o menor, não varre o vetor e pega o melhor)

**Análise Teórica X Análise Experimental**
  - Provas experinmentais e provas matemáticas

**Análise Assintótica X Análise Empírica**

## Análise Assintótica
  - Estimar o custo de se executar um algoritmo
  - Custo Assintótico, custo computacional ou complexidade assintótica
  - Encontrar uma equação (ou polinômio) que estime o custo do execução do método em função de algum critério pré definido
  - Foca no termo do polinômio de maior índice/grau (maior crescimento)
  - A partir do polinômio é possível determinar qual algoritmo é melhor
  - **Critérios:**
    - Define antes de começar a análise
    - Número de operações básicas
    - Número de indexações
    - Número de comparações
    - Número de acessos a disco
    - Número de Swaps
    - Espaço em memória
    - Número de acessos ao servidor
    - Apesar de cada operação ter seu custo específico, considera-se que todas as operações tem o mesmo custo
  - Ignora problemas pequenos e intermediários, concentrando-se em problemas extremamente grandes
    - Valores de *n* tendendo ao infinito
  - Fator de impacto do custo de um algoritmo está relacionado à variável de crescimento do custo e não valores fixos
    - Todos tem o mesmo **custo assintótico**, mas matemático são diferentes
    - $n^2$
    - $\frac{3n²}{2}$
    - $9999n^2$
    - $\frac{n^2}{1000}$
  
  **Divididos em 5 ordens:**
  - Ordem big O ou O grande ($\Omicron$)
    - Limite superior das funções (tal algoritmo não passa de um limite assintótico)
    - Funções assintóticamente não negativas
    - $f(n) >= 0$ para todo n suficientemente grande
    - $f(n) <= g(n)$ para todo n suficientemente grande
    - **n0**: elemento a partir do qual o problema não terá mais custo negativo
    - dadas duas grandes funções assintoticamente não negativas $f$ e $g$, dizemos que $f$ está na ordem O de $g$, e escrevemos...
    - $g(n)$ domina assintóticamente $f(n)$, ou que $g(n)$ é um limite assintótico superior para $f(n)$
    - $f=O(g)$
      - $O(n^2)$
    - $x^{12} - 10x^{11} - 3x^{8}+327x^7 => O(n^{12})$
    - O da esquerda pode ser menor que o da direito
      - $\frac{3n^2}{2}$ é $O(\frac{3n^3}{123})$
      - Entretanto não é interessante, pois está colocando um limite muito maior que o necessário
      - n Superestimado
    - Propriedades da função O
      - $f(n) = O(f(n))$
      - $c * O(f(n)) = O(f(n))$, sendo $c$ uma constante
      - $O(f(n)) + O(f(n)) = O(f(n))$: Como se tivesse a adição de uma constante
        - $O(f(n)) + O(f(n)) = 2O(f(n))$ => Corta a constante
      - $O(O(f(n))) = O(f(n))$
      - $O(f(n)) + O(g(n)) = O(Max(O(f(n)), O(g(n))))$: pega o pior entre os dois para avaliar o pior cenário
      - $O(f(n)) * O(g(n)) = O(f(n) * g(n))$

  - Ordem Omicron ($\omicron$)
    - Não tem igualdade entre as funções 
    - $f(n) < g(n)$
    - também tem o $n_0$
    - $g(n)$ sempre será o limite superior de $f(n)$, de modo que não haja local comum
    - Todas as prorpiedades da função O se aplicam, exceto a primeira ($f(n) = O(f(n))$)

  - Ordem Ômega ($\Omega$)
    - Primeira função é maior ou igual à segunda
    - $f(n) >= g(n)$
    - Criando um limite inferior (piso)
    - O melhor caso de execução dela é $x$

  - Ordem Ômega minsúculo ($\omega$)
    - Primeira função é maior que a segunda
    - $f(n) > g(n)$

  - Ordem Theta ($\Theta$)
    - As duas funções tem o mesmo custo assintótico
    - $f(n) = g(n)$
    - O algoritmo não depende do conjunto de entrada
    - O algoritmo sempre tem o mesmo comportamento
      - Média de todos os elementos de um vetor
    - Quando deseja comparar funções
      - $bubble_sort = \Theta(selection\_sort)$

  - A relação entre ordens de deve ser feita apenas entre duas funções
  - Resumo:
    - $f = \Omicron(g) -> f <= g$
    - $f = \omicron(g) -> f < g$
    - $f = \Omega(g) -> f >= g$
    - $f = \omega(g) -> f > g$
    - $f = \Theta(g) -> f = g$

  **Classes de Crescimento Assintótico**
  - $\Omicron(2^n)$
  - $\Omicron(n^3)$
  - $\Omicron(n^2)$
  - $\Omicron(n * log_2n)$
  - $\Omicron(n)$
  - $\Omicron(log_2n)$
  - $\Omicron(3)$

  **Apesar de equivalentes assintóticamente, as execuções reais podem ser bem discrepantes, mesmo que assintóticamente sejam equivalentes**

  - Provê apenas uma aproximação do real custo de execução do algoritmo
  - No entanto é necessário implementar e rodar o algorítmo para inúmeras entradas e avaliar seu custo

## Análise Empírica

  - É preciso definir o critério para ser avaliado
    - Memória, acesso a disco, swaps, tempo de execução, ...
  - É necessário implementar todas as soluções, do mesmo jeito
  - É necessário executar todos os algoritmos implementados para todas as entradas
  - Na análise então compara-se os balores obtidos durante as execuções
  - Conjunto de entrada
    - Variação de tamanho
    - Cenários: conjuntos aleatórios, parcialmente ordenados, ordenados, ...
  - Definição dos critérios de análise
    - tempo, requisições em servidor, acesso a disco, uso de memória, ...